{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################### STEP 1 -  의존성 로드 (초기에만 수행)######################################################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import KNNImputer\n",
    "import seaborn as sns # seaborn ref : https://greeksharifa.github.io/machine_learning/2019/12/05/Seaborn-Module/\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm;\n",
    "from patsy import dmatrices\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.width', 200)\n",
    "########################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################### STEP 2 - 데이터 임포팅 (초기에만 수행) ######################################################\n",
    "df = pd.read_csv(\"2018_SEL_FILLED.csv\") ## 파일리딩!\n",
    "#df.head()\n",
    "########################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################### STEP 3 - missing value filling(초기에만 수행) ###############################################\n",
    "#df=df.fillna(method='pad')\n",
    "\n",
    "# 범주형,이산형 변수인 경우 K-NN 수행\n",
    "# df = pd.read_excel('test.xlsx')\n",
    "# imputer = KNNImputer(n_neighbors=11)\n",
    "# data_filled = imputer.fit_transform(df)\n",
    "# data_filled = data_filled.astype('int64')\n",
    "# download = pd.DataFrame(data_filled).to_csv('filled.csv')\n",
    "\n",
    "# 연속형 변수인 경우 평균화기법 수행\n",
    "# df = pd.read_excel('test.xlsx')\n",
    "# data_filled = df.where(pd.notnull(df), df.median(), axis='columns')\n",
    "# download = pd.DataFrame(data_filled).to_csv('filled.csv')\n",
    "\n",
    "#df=df.drop(df.columns[0], axis='columns') #unnamed drop 없을경우 주석 처리\n",
    "df.isnull()\n",
    "########################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################### STEP 4 - Data에 대한 구조와 분석 (반복)######################################################\n",
    "df.hist(figsize=(20,20))#histogram plotting\n",
    "#sns.paiplot(df)\n",
    "df.head()\n",
    "df['HE_DM']\n",
    "########################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################### STEP 5 = 변수간 상관관계 및 다중공선성(VIF) 분석 (반복)#########################################\n",
    "\n",
    "# 변수간 상관관계 분석 - 1 \n",
    "t=df.corr(method='pearson') ## de1_pr 현재 당뇨병 유병 여부와 상관관계가 있는 변수들\n",
    "t.HE_DM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#변수간 상관관계 분석 - 2\n",
    "sns.heatmap(df.corr(),cmap=\"YlGnBu\")\n",
    "t.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VIF 산출\n",
    "df.dropna(inplace=True)\n",
    "df['intercept'] = 1\n",
    "lm = sm.OLS(df['HE_DM'], df)\n",
    "results = lm.fit()\n",
    "results.summary()\n",
    "\n",
    "# dmatrix 사용을 위한 모든 변수에 대한 string +\n",
    "cols = ''\n",
    "for col in list(df):\n",
    "    if(col=='sex'):\n",
    "        cols = cols + str(col)\n",
    "    else:\n",
    "        cols = cols + ' + ' + str(col)\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIF 지수 산출 - 10 이상인 친구들     \n",
    "y, X = dmatrices('HE_DM ~'+ cols , df, return_type = 'dataframe')\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "vif[\"features\"] = X.columns \n",
    "# ov_list=list(vif,list(vif['VIF Factor']>10))\n",
    "# print(ov_list)\n",
    "vif.head(200)\n",
    "over_vif_vars = vif[vif['VIF Factor']>10]\n",
    "over_vif_vars.head(200)\n",
    "########################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################## STEP 6 - 변수의 RF 모델에서 나온 중요도 산출 (반복) ########################################\n",
    "\n",
    "#인코딩\n",
    "df.loc[df[\"HE_DM\"] == 1,\"HE_DM\"] = 0\n",
    "df.loc[df[\"HE_DM\"] == 2,\"HE_DM\"] = 0\n",
    "df.loc[df[\"HE_DM\"] == 3,\"HE_DM\"] = 1\n",
    "df['HE_DM'].head(100)\n",
    "\n",
    "\n",
    "importance_list = []\n",
    "y = df.HE_DM # y축 설정\n",
    "x = df.drop(columns=[\"HE_DM\"]) #결과축 삭제 for predictions\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42) \n",
    "rnd_clf = RandomForestClassifier(n_estimators=500,n_jobs=-1)\n",
    "rnd_clf.fit(x_train,y_train)\n",
    "for name, score in zip(df[:],rnd_clf.feature_importances_): ## RF 적 방법으로 변수 중요도 추출\n",
    "    importance_list = importance_list + [name,score]\n",
    "    print(name, round(score,6))\n",
    "########################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### STEP 7 - step5, step 6의 결론을 토대로 변수 drop (반복)#########################################################\n",
    "df = df.drop(['변수명'],axis='columns')\n",
    "#df = df.drop(['DI4_pr','DM1_pr','DM2_pr','DM3_pr','DM4_pr','DJ2_pr','DJ4_pr','DC1_pr','DC2_pr','DC3_pr','DC3_pr','DC4_pr','DC5_pr','DC6_pr','DC7_pr','DF2_pr', 'DL1_pr', 'DJ0_pr', 'DJ6_pr', 'DH4_pr', 'DH3_pr', 'DH6_pr', 'DN1_pr', 'DK0_pr', 'DKNaN_pr', 'DK4_pr', 'LQ4_00', 'LQ1_sb', 'MH1_yr', 'MO1_wk','BD1','BS1_1','BS9_2','BE3_71','HE_mens','HE_prg','HE_DMfh1','HE_Usg','HE_Uket','HE_Ubld','HE_pef','HE_cough1'],axis='columns')\n",
    "########################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################### STEP 8 - 파이프라이닝 - 스케일링 / 모델링 결정 (반복) ##########################################\n",
    "\n",
    "## 데이터 분류 및 전처리\n",
    "# 축 분리#te\n",
    "Y = df.HE_DM # y축 설정\n",
    "X = df.drop(columns=[\"HE_DM\"]) #결과축 삭제 for predictions\n",
    "\n",
    "# 사이킷런 셋 분류 함수 : http://blog.naver.com/PostView.nhn?blogId=siniphia&logNo=221396370872\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.30, random_state=42) ## test.size 0.2~0.25 설정할거임\n",
    "\n",
    "\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC())])\n",
    "pipe.fit(X_train,Y_train)\n",
    "y_preds=pipe.predict(X_test)\n",
    "print(pipe.score(X_test,Y_test))\n",
    "plot_confusion_matrix(pipe,X_test,Y_test)\n",
    "########################################################################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
